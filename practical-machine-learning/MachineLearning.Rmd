---
title: "Practical Machine Learning - Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

The aim of this essay is to apply various machine learning techniques to predict the outcome of particular exercises, more in  detail we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants; for further details, see [1].

## Data Analisys and preparation
First of all, we start by loading the libraries we will use for our analisys:
```{r packages, echo = TRUE,warning=FALSE}
library(caret)
library(knitr)
library(kableExtra)
library(plyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(rpart)
library(rpart.plot)
library(rattle)
library(ggcorrplot)
```

We can now dowload the datasets provided and load them in two dataframe (train/test):

```{r download}
trainDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
testDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
```

A quick glance to the train/test dataset gives:

```{r dimTrain}
dim(trainDf)
```
```{r dimTest}
dim(testDf)
```

We can see that the train dataset has 19622 rows and 160 features, same number of features (as expected for the test dataset) and 20 rows; in the next paragraph we will try to reduce the number of variables in order to prepare the data to further analysis.


### Data Preparation
Firt of all, we will prepare the data for further analysis, we will first remove the ones that wouldn't contribute to the accuracy of the prediction (by common sense), than we will remove the columns with NAs, then the ones having a very low variance, and finally the ones showing a high correlation among each other.

#### Removing Variables that would not contribute to prediction's accuracy
Let's have a quick look to the train dataset:

```{r varToBeRemoved}
colnames(trainDf)
```

The first 7 variables seem to give very little information in terms of predicting the class variable, out of zealousness, we can create a new dataset for a simpler and more comprehensive view of those five variables:

```{r uselessCheck}
uselessVar <- trainDf[, (1:7)]
head(uselessVar)
```

The first variable seems to be an incremental identifier of each row, the user_name column is referring to the user of the device, the next three are clearly timestamps and the last 2 seems to refer to the execution context of the devide; we can start by removing those variables from our datasets:

```{r removeUseless}
trainDf <- trainDf[, -(1:7)]
testDf <- testDf[, -(1:7)]
```





#### Removing NAs

We can generate an outline of the NAs in a simple way:

```{r nas2}
sapply( trainDf, function(x) sum(is.na(x))) %>%
  kable(col.names = c("Number of NAs"), caption="Number of NAs per column") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",fixed_thead = T)) %>%
  scroll_box(width = "500px", height = "200px")
```

We can see that all the columns having NAs have a considerable amount of NAs (19216) covering more than  97% of the rows, we then can definitely remove those columns from both train and test datasets:

```{r remove_nas}
indexesToRemove <- which(colSums(is.na(trainDf))==19216) 
trainDf <- trainDf[,-indexesToRemove]
testDf <- testDf[,-indexesToRemove]
```

The number of columns is now:

```{r showVars}
ncol(trainDf)
```

We reduced the number of columns to 86.

#### Removing Variables with low variance
Time to have a look at the variance, for the sake of simplicity, we will just remove the variables having low variance; it must be noted tho that just removing variables with little or not variance is *not* necessarily a best practice (see [2] for details) but for the sake of this essay, we decide to just remove them.


```{r removeLowVariance}
nzv <- nearZeroVar(trainDf)
trainDf <- trainDf[, -nzv]
testDf <- testDf[, -nzv]
```

Looking at the number of columns now gives:

```{r columnsAfterLowVarRemoval}
ncol(trainDf)
```

We now have 53 variables (from the original 160).

#### Correlation among variables
Since correlation works only with numerical variables, we now create a new dataframe out of the train data set removing the only factor variable classe (which is the one that we want to predict).


```{r train correlation Data Frame}
trainCorrelationDf = trainDf[ , -which(names(trainDf) %in% c("classe"))]
```

Then we calculate the correlation among the variables and plot the correlation:

```{r train correlation calc}
trainCorrelationDf <- round(cor(trainCorrelationDf), 2)

ggcorrplot(trainCorrelationDf, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlation among variables", 
           ggtheme=theme_bw)
```

We can see from the plot above (all tho not clearly visible due to the number of variables) that some of the variables have some sort of correlation, we proceed now in removing the variables having high correlation (we arbitrarily set the value to be 90%) and check the number of columns:


```{r train correlation remove}
variablesToRemove = findCorrelation(trainCorrelationDf, cutoff=0.9)
trainDf = trainDf[,-c(variablesToRemove)]
testDf = testDf[,-c(variablesToRemove)]
ncol(trainDf)
```

We finally reduced the number of columns to 46 thus 45 variables as the last one (classe) is the one we want to predict.


## Machine Learning - based predictions.
In this paragraph we will apply three techniques:Classification Trees, Random Forests and Generalized Boosted Model. First of all, we will have to partition our train dataset into a further training set and another test set, to avoid confusion, we will create two new variables:

```{r train split}
set.seed(5382)
inTrain <- createDataPartition(y=trainDf$classe, p=0.7, list=F)
trainingSet <- trainDf[inTrain, ]
validationSet <- trainDf[-inTrain, ]
```

We will use the variables *trainingSet* and *validatonSet* in the reminder of our analysis, furthermore, we will use the following control to tune cross-validation:

```{r train set cross validation}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)

```

### Models' Training
In this paragraph we will train the chosen models and we will store their performances in one common variable for convenience; We start with Decision Trees: 

```{r train decision trees}
fit.decisionTree <- train(classe ~ ., data=trainingSet, method="rpart", trControl=fitControl)
```

Then we proceed with ramdom forest:

```{r train random forest}
fit.rf <- train(classe~., data=trainingSet, method="rf", trControl=fitControl)
```

And finally with theGeneralized Boosted Model:

```{r train gbm}
fit.gbm <- train(classe~., data = trainingSet, method = "gbm", trControl = fitControl,verbose = FALSE)
```

Time to check the performance of each of them:

```{r performance decision tree}
decisionTreePredictions <- predict(fit.decisionTree, newdata=validationSet)
confusionMatrix(validationSet$classe, decisionTreePredictions)
```

Decision Tree seems to have an accuracy of 49%.

```{r performance random forest}
randomForestPrediction <- predict(fit.rf, newdata=validationSet)
confusionMatrix(validationSet$classe, randomForestPrediction)
```

The random forest appraoch yeleded a 99.47% accuracy.

```{r performance gbm}
gbmPredictions <- predict(fit.gbm, newdata=validationSet)
confusionMatrix(validationSet$classe, gbmPredictions)
```

The last approach (GBM) reached a 96.3 % accuracy.


### Conclusions
So far the best appraoch is the Random Forest one, reaching an accuracy of 99.47%, we can now finally proceed in re-training the model on the full training set given at the beginning of this essay (as a reminder, we split the original training set):

```{r run rf on full set}
randomForest <- train(classe~., data=trainDf, method="rf", trControl=fitControl)


```

Then we can proceed with making the prediction that we are required to do on the original test set:

```{r run rf on full set1}
preds <- predict(randomForest, newdata=testDf)
```

The preds variable will contain the predictions for the 20 rows we were given in the rest data set.

## References
[1] http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

[2] https://www.r-bloggers.com/2014/03/near-zero-variance-predictors-should-we-remove-them/

