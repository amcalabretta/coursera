---
title: "MachineLearning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

The aim of this essay is to apply various machine learning techniques to predict the outcome of particular exercises, more in  detail we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants; for further details, see [1].

## Data Analisys and preparation
First of all, we start by loading the libraries we will use for our analisys:
```{r packages, echo = TRUE,warning=FALSE}
library(caret)
library(knitr)
library(kableExtra)
library(plyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(rpart)
library(rpart.plot)
library(rattle)
library(ggcorrplot)
```

We can now dowload the datasets provided and load them in two dataframe (train/test):

```{r download}
trainDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
testDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
```

A quick glance to the train/test dataset gives:

```{r dimTrain}
dim(trainDf)
```
```{r dimTest}
dim(testDf)
```

We can see that the train dataset has 19622 rows and 160 features, same number of features (as expected for the test dataset) and 20 rows; in the next paragraph we will try to reduce the number of variables in order to prepare the data to further analysis.


### Data Preparation
Firt of all, we will prepare the data for further analysis, we will first remove the ones that wouldn't contribute to the accuracy of the prediction (by common sense), than we will remove the columns with NAs, then the ones having a very low variance, and finally the ones showing a high correlation among each other.

#### Removing Variables that would not contribute to prediction's accuracy
Let's have a quick look to the train dataset:

```{r varToBeRemoved}
colnames(trainDf)
```

The first 7 variables seem to give very little information in terms of predicting the class variable, out of zealousness, we can create a new dataset for a simpler and more comprehensive view of those five variables:

```{r uselessCheck}
uselessVar <- trainDf[, (1:7)]
head(uselessVar)
```

The first variable seems to be an incremental identifier of each row, the user_name column is referring to the user of the device, the next three are clearly timestamps and the last 2 seems to refer to the execution context of the devide; we can start by removing those variables from our datasets:

```{r removeUseless}
trainDf <- trainDf[, -(1:7)]
testDf <- testDf[, -(1:7)]
```





#### Removing NAs

We can generate an outline of the NAs in a simple way:

```{r nas2}
sapply( trainDf, function(x) sum(is.na(x))) %>%
  kable(col.names = c("Number of NAs"), caption="Number of NAs per column") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",fixed_thead = T)) %>%
  scroll_box(width = "500px", height = "200px")
```

We can see that all the columns having NAs have a considerable amount of NAs (19216) covering more than  97% of the rows, we then can definitely remove those columns from both train and test datasets:

```{r remove_nas}
indexesToRemove <- which(colSums(is.na(trainDf))==19216) 
trainDf <- trainDf[,-indexesToRemove]
testDf <- testDf[,-indexesToRemove]
```

The number of columns is now:

```{r showVars}
ncol(trainDf)
```

We reduced the number of columns to 86.

#### Removing Variables with low variance
Time to have a look at the variance, for the sake of simplicity, we will just remove the variables having low variance; it must be noted tho that just removing variables with little or not variance is *not* necessarily a best practice (see [2] for details) but for the sake of this essay, we decide to just remove them.


```{r removeLowVariance}
nzv <- nearZeroVar(trainDf)
trainDf <- trainDf[, -nzv]
testDf <- testDf[, -nzv]
```

Looking at the number of columns now gives:

```{r columnsAfterLowVarRemoval}
ncol(trainDf)
```

We now have 53 variables (from the original 160).

#### Correlation among variables
Since correlation works only with numerical variables, we now create a new dataframe out of the train data set removing the only factor variable classe (which is the one that we want to predict).


```{r train correlation Data Frame}
trainCorrelationDf = trainDf[ , -which(names(trainDf) %in% c("classe"))]
```

Then we calculate the correlation among the variables and plot the correlation:

```{r train correlation calc}
trainCorrelationDf <- round(cor(trainCorrelationDf), 2)

ggcorrplot(trainCorrelationDf, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlation among variables", 
           ggtheme=theme_bw)
```

We can see from the plot above (all tho not clearly visible due to the number of variables) that some of the variables have some sort of correlation, we proceed now in


##Machine Learning - based predictions.
In this paragraph we will apply three techniques:Classification Trees, Random Forests and Generalized Boosted Model.

###Classification Trees

```{r recalculate}
set.seed(78282)
#classificationTree <- rpart(classe ~ ., data=trainDf, method="class")
#fancyRpartPlot(classificationTree)
```



###Random Forests

###Generalized Boosted Model



## References
[1] http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

[2] https://www.r-bloggers.com/2014/03/near-zero-variance-predictors-should-we-remove-them/

