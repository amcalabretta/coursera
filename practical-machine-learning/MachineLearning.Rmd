---
title: "MachineLearning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

The aim of this essay is to apply various machine learning techniques to predict the outcome of particular exercises, more in  detail we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants; for further details, see [1].

## Data Analisys and preparation
First of all, we start by loading the libraries we will use for our analisys:
```{r packages, echo = TRUE,warning=FALSE}
library(caret)
library(knitr)
library(kableExtra)
library(plyr)
library(ggplot2)
library(gridExtra)
library(grid)
library(rpart)
library(rpart.plot)
library(rattle)
```

We can now dowload the datasets provided and load them in two dataframe (train/test):

```{r download}
trainDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
testDf <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
```

A quick glance to the train/test dataset gives:

```{r dimTrain}
dim(trainDf)
```
```{r dimTest}
dim(testDf)
```

We can see that the train dataset has 19622 rows and 160 features, same number of features (as expected for the test dataset) and 20 rows; in the next paragraph we will try to reduce the number of variables in order to prepare the data to further analysis.


### Data Preparation
Firt of all, we will prepare the data for further analysis, we will first remove the ones that wouldn't contribute to the accuracy of the prediction (by common sense), than we will remove the columns with NAs, then the ones having a very low variance, and finally the ones showing a high correlation among each other.

#### Removing Variables that would not contribute to prediction's accuracy
Let's have a quick look to the train dataset:

```{r varToBeRemoved}
colnames(trainDf)
```

The first 5 variables seem to give very little information in terms of predicting the class variable, out of zealousness, we can create a new dataset for a simpler and more comprehensive view of those five variables:

```{r uselessCheck}
uselessVar <- trainDf[, (1:5)]
head(uselessVar)
```

The first variable seems to be an incremental identifier of each row, the user_name column is referring to the user of the device, the next three are clearly timestamps; we can start by removing those variables from our datasets:

```{r removeUseless}
trainDf <- trainDf[, -(1:5)]
testDf <- testDf[, -(1:5)]
```





#### Removing NAs

We can generate an outline of the NAs in a simple way:

```{r nas2}
sapply( trainDf, function(x) sum(is.na(x))) %>%
  kable(col.names = c("Number of NAs"), caption="Number of NAs per column") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",fixed_thead = T)) %>%
  scroll_box(width = "500px", height = "500px")
```

We can see that all the columns having NAs have a considerable amount of NAs (19216) covering more than  97% of the rows, we then can definitely remove those columns from both train and test datasets:

```{r remove_nas}
indexesToRemove <- which(colSums(is.na(trainDf))==19216) 
trainDf <- trainDf[,-indexesToRemove]
testDf <- testDf[,-indexesToRemove]
```

The number of columns is now:

```{r showVars}
ncol(trainDf)
```

We reduced the number of columns to 88.

#### Removing Variables with low variance
Time to have a look at the variance, for the sake of simplicity, we will just remove the variables having low variance; it must be noted tho that just removing variables with little or not variance is *not* necessarily a best practice (see [2] for details) but for the sake of this essay, we decide to just remove them.


```{r removeLowVariance}
nzv <- nearZeroVar(trainDf)
trainDf <- trainDf[, -nzv]
testDf <- testDf[, -nzv]
```

Looking at the number of columns now gives:

```{r columnsAfterLowVarRemoval}
ncol(trainDf)

```

We now have 54 variables (from the original 160).

#### Correlation among variables



##Machine Learning - based predictions.
In this paragraph we will apply three techniques:Classification Trees, Random Forests and Generalized Boosted Model.

###Classification Trees

```{r recalculate}
set.seed(78282)
#classificationTree <- rpart(classe ~ ., data=trainDf, method="class")
#fancyRpartPlot(classificationTree)
```



###Random Forests

###Generalized Boosted Model



## References
[1] http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

[2] https://www.r-bloggers.com/2014/03/near-zero-variance-predictors-should-we-remove-them/


